import os
import torch
import numpy as np
import cv2
from einops import rearrange, repeat
from PIL import ImageColor
from tqdm import tqdm
from moviepy import ImageSequenceClip, VideoFileClip, AudioFileClip

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•å’Œè®¾å¤‡
root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
device = 'cuda' if torch.cuda.is_available() else 'cpu'


def auto_downsample_ratio(h, w):
    """è‡ªåŠ¨è®¡ç®—ä¸‹é‡‡æ ·æ¯”ä¾‹ï¼Œä½¿è§†é¢‘çš„æœ€å¤§è¾¹ä¸è¶…è¿‡ 512 åƒç´ """
    return min(512 / max(h, w), 1)


def matting_video_to_images(video_path, output_folder, bg_color='white', batch_size=4, fp16=False, transparent=False):
    """
    å¯¹è§†é¢‘è¿›è¡ŒæŠ å›¾å¤„ç†ï¼Œä»…è¾“å‡ºå‰æ™¯å›¾åƒï¼ˆä¸ä¿å­˜ mask å›¾åƒï¼‰
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    cap.release()

    # åŠ è½½æ¨¡å‹
    model_path = os.path.join(root_dir, 'models', 'mobilenetv3', 'rvm_resnet50_fp32.torchscript')
    assert os.path.exists(model_path), f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{model_path}"
    model = torch.jit.load(model_path, map_location=device).eval()
    if fp16:
        model.half()

    cap = cv2.VideoCapture(video_path)
    os.makedirs(output_folder, exist_ok=True)

    pbar = tqdm(total=total_frames, desc=f"å¤„ç† {os.path.basename(video_path)}")

    rec = [None] * 4
    frame_idx = 0

    while True:
        frames = []
        for _ in range(batch_size):
            ret, frame = cap.read()
            if not ret:
                break
            # å›ºå®šåˆ†è¾¨ç‡ï¼š1080 x 1920
            frame = cv2.resize(frame, (1080, 1920))
            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0)

        if not frames:
            break

        video_frames = torch.from_numpy(np.stack(frames)).float()
        video_frames = rearrange(video_frames, "n h w c -> n c h w").to(device)
        if fp16:
            video_frames = video_frames.half()

        with torch.no_grad():
            fgrs, phas, *rec = model(video_frames, *rec, auto_downsample_ratio(height, width))
            masks = phas.gt(0).float()  # è½¬ä¸º float é¿å… bool å‡æ³•é”™è¯¯

            if transparent:
                # é€æ˜èƒŒæ™¯ï¼šå‰æ™¯ + mask ä½œä¸º alpha é€šé“
                fgrs = fgrs * masks + (1.0 - masks) * 1.0
            else:
                # å›ºå®šé¢œè‰²èƒŒæ™¯
                bg = torch.Tensor(ImageColor.getrgb(bg_color)[:3]).float() / 255.
                bg = repeat(bg, "c -> n c h w", n=fgrs.shape[0], h=1, w=1).to(device)
                if fp16:
                    bg = bg.half()
                fgrs = fgrs * masks + (1.0 - masks) * 1.0

        fgrs = rearrange(fgrs.float().cpu(), "n c h w -> n h w c").numpy()

        for i, fgr in enumerate(fgrs):
            fgr = (fgr * 255).astype(np.uint8)
            if transparent:
                mask = (masks[i].cpu().numpy() * 255).astype(np.uint8)

                # é€šç”¨æ–¹å¼ï¼šå…ˆé™ç»´ï¼Œå†æ‰©å±•é€šé“ç»´åº¦
                if mask.ndim == 3:
                    mask = mask.squeeze(0)[..., None]  # å˜ä¸º [H, W, 1]
                elif mask.ndim == 4:
                    mask = mask.squeeze(0)[..., 0, None]  # å˜ä¸º [H, W, 1]

                rgba = np.concatenate([fgr, mask], axis=-1)  # shape: [H, W, 4]
                cv2.imwrite(os.path.join(output_folder, f"frame_{frame_idx:05d}_rgba.png"),
                            cv2.cvtColor(rgba, cv2.COLOR_RGBA2BGRA))
            else:
                cv2.imwrite(os.path.join(output_folder, f"frame_{frame_idx:05d}_fgr.png"),
                            cv2.cvtColor(fgr, cv2.COLOR_RGB2BGR))
            frame_idx += 1
            pbar.update(1)

    cap.release()
    pbar.close()
    print("âœ… æŠ å›¾å®Œæˆï¼Œå›¾åƒåºåˆ—å·²ä¿å­˜è‡³:", output_folder)

    # è¿”å›è§†é¢‘åŸºæœ¬ä¿¡æ¯ï¼Œä¾›åç»­ä½¿ç”¨
    return {
        'total_frames': total_frames,
        'fps': fps
    }


def synthesize_video_from_images(output_folder, video_info, video_path, transparent=False):
    """
    ä»å›¾åƒåºåˆ—ç”Ÿæˆè§†é¢‘ï¼Œå¹¶æå–åŸå§‹è§†é¢‘éŸ³é¢‘è¿›è¡Œåˆæˆ
    """
    fps = video_info['fps']
    file_name = os.path.basename(video_path)

    if transparent:
        image_files = sorted(
            [os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith("_rgba.png")],
            key=lambda x: int(x.split("_")[-2])
        )
        output_video_path = os.path.join(os.path.dirname(output_folder), f"{file_name}_rgba.mp4")
    else:
        image_files = sorted(
            [os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith("_fgr.png")],
            key=lambda x: int(x.split("_")[-2])
        )
        output_video_path = os.path.join(os.path.dirname(output_folder), f"{file_name}_fgr.mp4")

    if image_files:
        print("ğŸ¬ æ­£åœ¨ç”Ÿæˆè§†é¢‘...")
        clip = ImageSequenceClip(image_files, fps=fps)
        clip.write_videofile(output_video_path, codec="libx264", audio_codec="aac", logger=None)

        # æå–åŸå§‹éŸ³é¢‘
        print("ğŸµ æ­£åœ¨æå–åŸå§‹è§†é¢‘éŸ³é¢‘...")
        audio_path = os.path.join(output_folder, "extracted_audio.aac")
        original_clip = VideoFileClip(video_path)
        if original_clip.audio:
            original_clip.audio.write_audiofile(audio_path, codec="aac")

        if os.path.exists(audio_path):
            print("ğŸ”Š æ­£åœ¨åˆæˆéŸ³é¢‘åˆ°è§†é¢‘...")
            final_video_path = output_video_path.replace(".mp4", "_with_audio.mp4")
            final_clip = VideoFileClip(output_video_path)
            final_clip.audio=AudioFileClip(audio_path)
            final_clip.write_videofile(final_video_path, codec="libx264", audio_codec="aac", logger=None)

        print("ğŸ‰ è§†é¢‘åˆæˆå®Œæˆ:", final_video_path)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å›¾åƒåºåˆ—ï¼Œæ— æ³•ç”Ÿæˆè§†é¢‘ã€‚")


def process_videos_in_folder(input_folder, output_folder, bg_color='white', batch_size=8, fp16=False, transparent=False):
    """
    éå†æ–‡ä»¶å¤¹ä¸­çš„è§†é¢‘å¹¶è¿›è¡ŒæŠ å›¾å¤„ç†ï¼ˆä¸ä¿å­˜ mask å›¾åƒï¼‰
    """
    supported_exts = ['.mp4', '.avi', '.mov', '.mkv']
    for filename in os.listdir(input_folder):
        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext in supported_exts:
            video_path = os.path.join(input_folder, filename)
            file_name = os.path.splitext(filename)[0]
            out_dir = os.path.join(output_folder, file_name)

            print(f"\nğŸ¥ å¼€å§‹å¤„ç†è§†é¢‘: {filename}")
            video_info = matting_video_to_images(video_path, out_dir, bg_color, batch_size, fp16, transparent)
            synthesize_video_from_images(out_dir, video_info, video_path, transparent)


if __name__ == '__main__':
    input_folder = r'D:\PycharmProjects\MutilsSingSong\assets\51'  # è¾“å…¥è§†é¢‘æ–‡ä»¶å¤¹
    output_folder = r'D:\PycharmProjects\MutilsSingSong\assets\51\matting'  # è¾“å‡ºæ–‡ä»¶å¤¹

    matting_args = dict(
        bg_color='black',  # å¯é€‰ black / white / transparent
        batch_size=4,
        fp16=True,  # è‹¥ GPU æ”¯æŒ FP16 æ¨èå¼€å¯
        transparent=False  # æ˜¯å¦è¾“å‡ºé€æ˜èƒŒæ™¯å›¾åƒï¼ˆRGBAï¼‰
    )

    process_videos_in_folder(input_folder, output_folder, **matting_args)
